<!DOCTYPE html>
<html>
  <head>
    <title>Deploying Machine Learning Models in Python | PyCon HK 2018</title>
    <meta charset="utf-8">
    <link rel='stylesheet' href='style.css' type='text/css' media='all' />
  </head>
  <body>
    <textarea id="source">

class: center, middle

# Deploying Machine Learning Models<br/>in Python
<br/><br/>

### Albert Au Yeung

#### PyCon HK 2018<br/>23th November, 2018

---

# What is this Talk About?

* The need to **deploy** machine learning models
* Different kinds of **workflows**
* Common **strategies**
* **Options** in Python
* Other **considerations**
* NOT about using docker/kubernetes

---

# The "Kaggle" Way of Machine Learning

* [Kaggle](https://www.kaggle.com/): a Website that host machine learning competitions
* You train machine learning models and generate predictions on the testing data
* Everything is done **offline**

<center>
  <img src="img/kaggle.png" width="70%"/>
</center>

---

# Machine Learning Applications

* In practice, generating predictions is only a **small part** of a machine learning project
* Consider a system that uses machine learning to recognize hand-written letters

<center>
  <img src="img/ml-system-example.png" width="90%"/>
</center>

---

# Common ML Systems Workflow

* There are several common **workflows** for machine learning systems
    1. Train offline ➤ Predict offline ➤ Store predictions in DB
    2. Train offline ➤ Embed model in a device ➤ Predict online
    3. Train offline ➤ Make model available as a service ➤ Predict online
* Notes:
    - **Offline**<br/>separate from a production system; does not have to be completed in real time
    - **Online**<br/>part of a production system; perform tasks in real time

---

# Common ML Systems Workflow (1)

* Train offline ➤ Predict offline ➤ Store predictions in DB
* Example: **Recommender systems**
    - A model is trained **offline**
    - For each user, generate (pre-compute) a list of recommended items, store in **database**
    - When the user visits the Website, return the list of items

<center>
    <img src="img/amazon-recommendations.png" width="100%"/>
</center>


---

class: equal-split

# Common ML Systems Workflow (2)

.column-left[
* Train offline ➤ Embed model in a device ➤ Predict online
* Example: **Object detection using a drone**
    - A model is trained offline
    - The model together with other processing logic are downloaded to the drone's computer
    - The drone detects objects while it is in operation
]
.column-right[
<center>
    <img src="img/drone-object-detection.png" width="100%"/>
</center>
]

---

# Common ML Systems Workflow (3)

* Train offline ➤ Make model available as a service ➤ Predict online
* Example: **Spam E-mail detection**
    - A classifier is trained offline with spam and non-span emails
    - Deployed as a service to serve users or other components in the system

<center>
    <img src="img/ml-service-example.png" width="85%"/>
</center>

---

# Common ML Systems Workflow

* In (2) and (3), we need to think about how to **deploy** a machine learning model
* Definition of **deploy**:

<div class="quote">
  To place some resources into a position so as to be ready for action or use
</div>

* In this talk, we will focus on **Use Case (3)**
* How to make machine learning models **available** to other users/systems?

---

class: equal-split

# Common Strategies

.column-left[
* **Persist** model in a **standard** format
* **Different languages** for development and production
]
.column-right[
* Serve models in **micro-services**
* The **same** language and packages used in development and production
]

<center>
  <img src="img/common-strategies.png" width="75%"/>
</center>
  

---

class: split-60

# Persist Models in a Standard Format

.column-left[
```python
from sklearn.linear_model import LogisticRegression
from sklearn.feature_extraction.text import CountVectorizer
from sklearn2pmml import sklearn2pmml
from sklearn2pmml.pipeline import PMMLPipeline

# Load data ...

# Create pipeline and fit model
pipeline = PMMLPipeline([
  ("vec", CountVectorizer()),
  ("clf", LogisticRegression())
])
pipeline.fit(X, y)

# Write model in PMML format
sklearn2pmml(pipeline, "model.pmml", with_repr=True)
```
]
.column-right[
* **Predictive Model Markup Language (PMML)**
  - XML-based file format for model interchange
  - scikit-learn to PMML:<br/>[sklearn2pmml](https://github.com/jpmml/sklearn2pmml)
  - LightGBM to PMML:<br/>[jpmml-lightgbm](https://github.com/jpmml/jpmml-lightgbm)
* References: [Converting Scikit-Learn to PMML](https://www.slideshare.net/VilluRuusmann/converting-scikitlearn-to-pmml)
]

---

# Serve Models in Micro-services

* Models are persisted in a certain format specific to the language in development<br/>(e.g. using `sklearn.externals.joblib`)
* The model (or even the module to load, pre-process and generate predictions) is **wrapped** in a **micro-service** that expose **endpoints** to receive requests from **clients**

<center>
    <img src="img/micro-service.png" width="90%"/>
</center>

---

# Options in Python

* XML-RPC
* HTTP REST micro-services
    - Flask: [http://flask.pocoo.org/](http://flask.pocoo.org/)
    - Bottle: [https://bottlepy.org/](https://bottlepy.org/docs/dev/)
    - Falcon: [https://falconframework.org/](https://falconframework.org/index.html)
    - Vibora: [https://github.com/vibora-io/vibora](https://github.com/vibora-io/vibora)
    - AIOHTTP: [https://aiohttp.readthedocs.io/en/stable/](https://aiohttp.readthedocs.io/en/stable/)
* Asynchronous Messaging
    - Kafka: [https://kafka.apache.org/](https://kafka.apache.org/)
    - RabbitMQ: [https://www.rabbitmq.com/](https://www.rabbitmq.com/)
    - Redis: [https://redis.io/](https://redis.io/)

---

# Wrapping Model Prediction in a Class

* Generating predictions can involve **pre-processing** and **post-processing**
* More convenient if everything is wrapped inside a **class**

```python
class TextClassifier(object):
    """A Class wrapping the ML model"""

    def __init__(self):
        # Load the persisted model into memory
        self.model = joblib.load("model.pkl")
    
    def train(self):
        # Model training
        # ...
    
    def predict(self, x):
        y = self.model.predict([x])
        return y[0]
```

---

# Tensorflow Models

* For Tensorflow models, we need to keep a reference to the graph during initialization

```python
import tensorflow as tf
from keras.applications.resnet50 import ResNet50

class ImageClassifier(object):

    def __init__(self):
        self.graph = tf.get_default_graph()
        self.model = ResNet50(weights='imagenet')
    
    def preprocess(self, img):
        # preprocess the input image
        # ...

    def predict(self, img):
        x = self.preprocess(img)
        with self.graph.as_default():
            predictions = self.model.predict(x)
        ...
```

---

class: equal-split

# XML-RPC

* [XML-RPC](https://en.wikipedia.org/wiki/XML-RPC): Remote procedure call based on HTTP and XML file format
* The most straight-forward way if the **clients are also written in Python**

.column-left[
* Server

```python
from xmlrpc.server import \
    SimpleXMLRPCServer
from model import TextClassifier

clf = TextClassifer()

address = ("localhost", 8000)
server = SimpleXMLRPCServer(address)
server.register_function(
    clf.predict, "predict")

server.serve_forever()
```
]
.column-right[
* Client

```python
from xmlrpc.client import ServerProxy

address = ("localhost", 8000)

# Create a proxy to the XML-RPC server
with ServerProxy(address) as server:
    server.predict("Hello!")
```
]

---

# Flask

* A popular WSGI Web framework for creating HTTP APIs

```python
from flask import Flask, current_app, request, jsonify
from model import TextClassifier

app = Flask(__name__)         # Create a Flask app
app.model = TextClassifier()  # Load model into the app

# Define the HTTP API for prediction
@app.route('/predict', methods=['POST'])
def predict():
    d = request.get_json()
    prediction = current_app.model.predict(d['x'])
    return jsonify(result=prediction)

# Start the Flask internal Web server
app.run()
```

---

# Deploying

* Flask is a **WSGI** Web Framework, it can be deployed using [Gunicorn](https://gunicorn.org/)
* Gunicorn uses a **pre-fork worker** model (creates copies of your application)
* `$ gunicorn app:app -b localhost:8000 -w 4`

<center>
    <img src="img/gunicorn.png" width="50%"/>
</center>

---

# Falcon

* A Framework that focuses on REST APIs
* [Falcon vs. Flask — Which one to pick to create a scalable deep learning REST API](https://medium.com/idealo-tech-blog/falcon-vs-flask-which-one-to-pick-to-create-a-scalable-deep-learning-rest-api-adef647ebdec)

```python
import falcon
from model import TextClassifier

class Handler(object):

    def __init__(self):
        self.model = TextClassifier()

    def on_post(self, req, resp):
        data = json.loads(req.stream.read())
        resp.media = {"result": self.model.predict(data['x'])}

app = falcon.API()
app.add_route('/predict', Handler())
app.run()
```

---

# Vibora

* Asynchronous HTTP client/server framework (requires Python 3.6+)
* API very similar to Flask

```python
from vibora import Vibora, Request
from vibora.responses import JsonResponse
from model import TextClassifier

app = Vibora()  # Create an Vibora application
app.add_component(TextClassifer())  # Add a globally available component

@app.route('/predict', methods=['POST'])
async def predict(request: Request):
    data = await request.json()
    model = app.get_component(TextClassifier)  # Get reference to the model
    prediction = model.predict(data['x'])
    return JsonResponse({"result": prediction})
```

---

# AIOHTTP

* Framework for implementing Asychronous HTTP client/server on top of `asyncio`

```python
from aiohttp import web
from model import TextClassifier

async def handle(request):
    data = await request.json()
    prediction = request.app['model'].predict(data['input'])
    return web.json_response({"result": prediction})

app = web.Application()
app.router.add_post('/predict', handle)
app["model"] = TextClassifier()

web.run_app(app, host='localhost', port=8000)
```

---

class: equal-split

# Comparing Web Frameworks

* Some benchmarking results can be found [here](http://klen.github.io/py-frameworks-bench/) or [here](https://github.com/vibora-io/benchmarks)
* For ML services, the choice of framework seems not too important, because most of the time will be spent on data **processing** and **inference**

.column-left[
<center>
    <img src="img/benchmark.png" width="100%"/>
    <br/>
    Simple request and response<br/>
    http://klen.github.io/py-frameworks-bench/
</center>
]
.column-right[
<center>
    <img src="img/vibora-stats.png" width="70%"/>
    <br/>
    POST JSON data + Redis API Call<br/>
    https://github.com/vibora-io/benchmarks
</center>
]

---

# Disadvantage of Using HTTP APIs

* HTTP REST APIs are simple to implement, however the process is **synchronous**
* The client must **wait** until the ML service has finished the process of generating predictions
* In a complex system involving a lot of components, this may not be efficient

<center>
    <img src="img/http-round-trip.png" width="65%"/>
</center>

---

# Asynchronous Messaging

* If components are relatively independent, using **asynchronous messaging** can make more efficient use of the services' resources
* Send requests and responses to a **message broker** instead of directly to the services
<br/>

<center>
    <img src="img/message-broker.png" width="65%"/>
</center>

* Options:
    - Redis [https://redis.io/](https://redis.io/)
    - RabbitMQ: [https://www.rabbitmq.com/](https://www.rabbitmq.com/)
    - Kafka: [https://kafka.apache.org/](https://kafka.apache.org/)

---

class: equal-split

# Redis

* Redis is a key/value cache, but can also be used as a **message queue**

.column-left[
* Client Side

```python
from redis import StrictRedis

# Get a connection to Redis
queue = StrictRedis(host='localhost',
                    port=6379)

# Create a message
message = "Hello!".encode("utf-8")

# Send the message to a channel
# named "prediction"
queue.publish("prediction", message)
```
]
.column-right[
* ML Service

```python
from redis import StrictRedis
from model import TextClassifier

queue = StrictRedis(host='localhost',
                    port=6379)
pubsub = queue.pubsub()
pubsub.subscribe('prediction')

while True:
    x = p.get_message()["data"]
    y_pred = model.predict(x)
    # send result back ...
```
]

---

class: equal-split

# Kafka

* A scalable and distributed message queue
* Two Python packages available: [kafka-python](https://github.com/dpkp/kafka-python) and [confluent-kafka](https://github.com/confluentinc/confluent-kafka-python)

.column-left[
* Producer of messages

```python
from kafka import KafkaProducer

# Create a message producer
address = 'localhost:1234'
producer = KafkaProducer(
    bootstrap_servers=address)

# Send message to a topic
producer.send('prediction', # topic
              '{"x": "Hello!"}') # msg
...
```
]
.column-right[
* Consumer of messages

```python
from kafka import KafkaConsumer

# Create a consumer of a topic
consumer = KafkaConsumer('prediction')

# Get message from broker:
for msg in consumer:
    # Decode message
    content = msg.value.decode("utf-8")
    data = json.loads(content)
    ...
```
]

---

# Scaling ML Services

* Nowadays micro-services are usually deployed as [docker containers](https://www.docker.com/) managed using systems such as [Kubernetes](https://kubernetes.io/)
* **Scaling** involves creating multiple containers, each container running a copy of the ML service
* Challenge: How to configure **resources allocated to each container**?
<br/><br/>

<center>
    <img src="img/docker-deploy.png" width="95%"/>
</center>

---

# Scaling ML Services

* Some models can benefit from **multiple cores** using multi-threading / multi-processing, e.g.:
    - scikit-learn's [RandomForestClassifier](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html)
    - Facebook's [fastText](https://fasttext.cc/)
    - Deep learning models built using Tensorflow or PyTorch
* Some models are huge and consume a lot of **RAM**
* Some models can only run on **single cores**
* Using GPUs, sometimes **batch processing** can be faster

---

# Single vs. Multiple Workers

* Example: using **Flask** and **Gunicorn** to deploy a Tensorflow model
* Tensorflow models can utilize **multiple cores** during inference
* Would be better to scale using multiple containers, each allocated with multiple cores, rather than having multiple workers in a single container

<br/><br/>
<center>
    <img src="img/gunicorn-workers.png" width="80%"/>
</center>

---

# Summary

* When using a model in a **production system**, in addition to the performance<br/>(i.e. accuracy / precision / recall of the model, we need to consider:
1. **Model Size** ➤ would it be too big to be copied around?
2. **Memory** ➤ how much memory will it take up after loaded?
3. **CPU** ➤ how much CPU resources needed to generate a prediction?
4. **Time to Predict** ➤ time requried to generate a prediction
5. **Preprocessing** ➤ how complicated are the preprocessing steps?

---

class: center, middle

# Thank You!

### http://www.albertauyeung.com<br/>albertauyeung@gmail.com

### Slides Avaliable at:<br/> http://talks.albertauyeung.com/pycon2018-deploy-ml-models/


    </textarea>
    <script src="https://remarkjs.com/downloads/remark-latest.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS_HTML&delayStartupUntil=configured" type="text/javascript"></script>
    <script>
      var slideshow = remark.create({
        // Set the slideshow display ratio
        ratio: '16:9',

        // Customize slide number label, either using a format string..
        slideNumberFormat: '%current% / %total%',
        
        // Enable or disable counting of incremental slides in the slide counting
        countIncrementalSlides: true,

        highlightStyle: "github"
        });
      // Setup MathJax
      MathJax.Hub.Config({
          tex2jax: {
          skipTags: ['script', 'noscript', 'style', 'textarea', 'pre']
          }
      });

      MathJax.Hub.Configured();
    </script>

<script>
	(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
	(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
	m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
	})(window,document,'script','//www.google-analytics.com/analytics.js','ga');
	ga('create', 'UA-30554548-1', 'auto');
	ga('send', 'pageview');
</script>

  </body>
</html>
